{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-06T14:58:46.373140Z",
     "iopub.status.busy": "2025-03-06T14:58:46.372748Z",
     "iopub.status.idle": "2025-03-06T14:58:50.243979Z",
     "shell.execute_reply": "2025-03-06T14:58:50.243262Z",
     "shell.execute_reply.started": "2025-03-06T14:58:46.373108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T14:58:50.245801Z",
     "iopub.status.busy": "2025-03-06T14:58:50.245218Z",
     "iopub.status.idle": "2025-03-06T14:58:50.415971Z",
     "shell.execute_reply": "2025-03-06T14:58:50.414888Z",
     "shell.execute_reply.started": "2025-03-06T14:58:50.245745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160, 100, 20])\n",
    "        upper_red2 = np.array([179, 255, 255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1\n",
    "        \n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "        \n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = np.expand_dims(full_mask, axis=-1)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.read_mask(label_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].float()\n",
    "            label = label.permute(2, 0, 1)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T14:58:50.417752Z",
     "iopub.status.busy": "2025-03-06T14:58:50.417518Z",
     "iopub.status.idle": "2025-03-06T14:58:50.436459Z",
     "shell.execute_reply": "2025-03-06T14:58:50.435716Z",
     "shell.execute_reply.started": "2025-03-06T14:58:50.417732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic convolution block: Conv2d -> BatchNorm -> ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    def __init__(self, num_classes=3, H=480, W=480):\n",
    "        \"\"\"\n",
    "        UNet++ with ResNet-50 encoder and nested skip connections.\n",
    "        Parameters:\n",
    "            num_classes: Number of segmentation classes.\n",
    "            H, W: Input image dimensions.\n",
    "        \"\"\"\n",
    "        super(UNetPlusPlus, self).__init__()\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        self.x00 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)  # [B, 64, 120, 120]\n",
    "        self.x10 = resnet.layer1   # [B, 256, 120, 120]\n",
    "        self.x20 = resnet.layer2   # [B, 512, 60, 60]\n",
    "        self.x30 = resnet.layer3   # [B, 1024, 30, 30]\n",
    "        self.x40 = resnet.layer4   # [B, 2048, 15, 15]\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        self.conv01 = ConvBlock(in_channels=64 + 256, out_channels=64)\n",
    "        self.conv11 = ConvBlock(in_channels=256 + 512, out_channels=256)\n",
    "        self.conv21 = ConvBlock(in_channels=512 + 1024, out_channels=512)\n",
    "        self.conv31 = ConvBlock(in_channels=1024 + 2048, out_channels=1024)\n",
    "\n",
    "        self.conv02 = ConvBlock(in_channels=64 + 64 + 256, out_channels=64)\n",
    "        self.conv12 = ConvBlock(in_channels=256 + 256 + 512, out_channels=256)\n",
    "        self.conv22 = ConvBlock(in_channels=512 + 512 + 1024, out_channels=512)\n",
    "        \n",
    "        self.conv03 = ConvBlock(in_channels=64 + 64 + 64 + 256, out_channels=64)\n",
    "        self.conv13 = ConvBlock(in_channels=256 + 256 + 256 + 512, out_channels=256)\n",
    "        \n",
    "        self.conv04 = ConvBlock(in_channels=64 + 64 + 64 + 64 + 256, out_channels=64)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x00 = self.x00(x)   # [B, 64, 120, 120]\n",
    "        x10 = self.x10(x00) # [B, 256, 120, 120]\n",
    "        x20 = self.x20(x10) # [B, 512, 60, 60]\n",
    "        x30 = self.x30(x20) # [B, 1024, 30, 30]\n",
    "        x40 = self.x40(x30) # [B, 2048, 15, 15]\n",
    "        \n",
    "        x01 = self.conv01(torch.cat([x00, x10], dim=1))\n",
    "        x11 = self.conv11(torch.cat([x10, self.upsample(x20)], dim=1))\n",
    "        x21 = self.conv21(torch.cat([x20, self.upsample(x30)], dim=1))\n",
    "        x31 = self.conv31(torch.cat([x30, self.upsample(x40)], dim=1))\n",
    "\n",
    "        x02 = self.conv02(torch.cat([x00, x01, x11], dim=1))\n",
    "        x12 = self.conv12(torch.cat([x10, x11, self.upsample(x21)], dim=1))\n",
    "        x22 = self.conv22(torch.cat([x20, x21, self.upsample(x31)], dim=1))\n",
    "\n",
    "        x03 = self.conv03(torch.cat([x00, x01, x02, x12], dim=1))\n",
    "        x13 = self.conv13(torch.cat([x10, x11, x12, self.upsample(x22)], dim=1))\n",
    "        \n",
    "        x04 = self.conv04(torch.cat([x00, x01, x02, x03, x13], dim=1))\n",
    "        \n",
    "\n",
    "        output = self.final_conv(x04) \n",
    "        output = F.interpolate(output, size=(self.H, self.W), mode=\"bilinear\", align_corners=True)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T14:58:50.437652Z",
     "iopub.status.busy": "2025-03-06T14:58:50.437322Z",
     "iopub.status.idle": "2025-03-06T14:59:05.196795Z",
     "shell.execute_reply": "2025-03-06T14:59:05.195613Z",
     "shell.execute_reply.started": "2025-03-06T14:58:50.437620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ce45bc0501a9>\u001b[0m in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = F.softmax(preds, dim=1) \n",
    "        targets_one_hot = F.one_hot(targets, num_classes=preds.shape[1]).permute(0, 3, 1, 2) \n",
    "\n",
    "        intersection = (preds * targets_one_hot).sum(dim=(2, 3)) \n",
    "        union = (preds + targets_one_hot).sum(dim=(2, 3)) - intersection\n",
    "        iou = (intersection + self.eps) / (union + self.eps)\n",
    "        return 1 - iou.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.iou_loss = IoULoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        ce = self.ce_loss(preds, targets)\n",
    "        iou = self.iou_loss(preds, targets)\n",
    "        return self.alpha * ce + (1 - self.alpha) * iou\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.VerticalFlip(p=0.4),\n",
    "    A.RandomGamma(gamma_limit=(70, 130), p=0.2),\n",
    "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "lr = 0.005\n",
    "batch_size = 8\n",
    "epochs = 150\n",
    "in_channels = 3 \n",
    "out_channels = 3  \n",
    "H, W = 480, 480 \n",
    "\n",
    "model = UNetPlusPlus(num_classes=out_channels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = CombinedLoss(alpha=0.5) \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160, 100, 20])\n",
    "        upper_red2 = np.array([179, 255, 255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1 \n",
    "        \n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2  \n",
    "        \n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.read_mask(label_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].long() \n",
    "        \n",
    "        return image, label\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    img_dir='/kaggle/input/bkai-igh-neopolyp/train/train',\n",
    "    label_dir='/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt',\n",
    "    resize=(H, W),\n",
    "    transform=train_transform,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "\n",
    "    for images, masks in progress_bar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-06T14:59:05.197274Z",
     "iopub.status.idle": "2025-03-06T14:59:05.197560Z",
     "shell.execute_reply": "2025-03-06T14:59:05.197450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/model_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-06T14:59:05.198112Z",
     "iopub.status.idle": "2025-03-06T14:59:05.198423Z",
     "shell.execute_reply": "2025-03-06T14:59:05.198263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, image_path, device):\n",
    "    model.eval()\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformed = A.Compose([\n",
    "        A.Resize(480, 640),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])(image=image)\n",
    "    \n",
    "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prediction = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(prediction, cmap=\"jet\")\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "model = UNetPlusPlus(num_classes=3).to(device) \n",
    "model.load_state_dict(torch.load(\"/kaggle/working/model_weights.pth\", map_location=device))\n",
    "model.eval()\n",
    "device=\"cuda\"\n",
    "infer(model, \"/kaggle/input/bkai-igh-neopolyp/train/train/0081835cf877e004e8bfb905b78a9139.jpeg\", device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
