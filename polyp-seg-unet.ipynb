{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-06T09:48:29.077369Z",
     "iopub.status.busy": "2025-03-06T09:48:29.076799Z",
     "iopub.status.idle": "2025-03-06T09:48:32.962557Z",
     "shell.execute_reply": "2025-03-06T09:48:32.961824Z",
     "shell.execute_reply.started": "2025-03-06T09:48:29.077320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:48:32.963939Z",
     "iopub.status.busy": "2025-03-06T09:48:32.963526Z",
     "iopub.status.idle": "2025-03-06T09:48:32.973930Z",
     "shell.execute_reply": "2025-03-06T09:48:32.972911Z",
     "shell.execute_reply.started": "2025-03-06T09:48:32.963915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160, 100, 20])\n",
    "        upper_red2 = np.array([179, 255, 255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1\n",
    "        \n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "        \n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = np.expand_dims(full_mask, axis=-1)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.read_mask(label_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].float()\n",
    "            label = label.permute(2, 0, 1)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:48:32.975776Z",
     "iopub.status.busy": "2025-03-06T09:48:32.975553Z",
     "iopub.status.idle": "2025-03-06T09:48:33.156953Z",
     "shell.execute_reply": "2025-03-06T09:48:33.156265Z",
     "shell.execute_reply.started": "2025-03-06T09:48:32.975756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained=True):\n",
    "        super(UNet, self).__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None)\n",
    "        self.encoder = create_feature_extractor(resnet, return_nodes={\n",
    "            \"relu\": \"enc1\",          # (B, 64, H, W)\n",
    "            \"layer1\": \"enc2\",        # (B, 256, H/2, W/2)\n",
    "            \"layer2\": \"enc3\",        # (B, 512, H/4, W/4)\n",
    "            \"layer3\": \"enc4\",        # (B, 1024, H/8, W/8)\n",
    "            \"layer4\": \"bridge\"       # (B, 2048, H/16, W/16)\n",
    "        })\n",
    "        self.decoder4 = DoubleConv(2048 + 1024, 1024)\n",
    "        self.decoder3 = DoubleConv(1024 + 512, 512)\n",
    "        self.decoder2 = DoubleConv(512 + 256, 256)\n",
    "        self.decoder1 = DoubleConv(256 + 64, 64)\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        enc1 = features[\"enc1\"]  # (B, 64, H, W)\n",
    "        enc2 = features[\"enc2\"]  # (B, 256, H/2, W/2)\n",
    "        enc3 = features[\"enc3\"]  # (B, 512, H/4, W/4)\n",
    "        enc4 = features[\"enc4\"]  # (B, 1024, H/8, W/8)\n",
    "        bridge = features[\"bridge\"]  # (B, 2048, H/16, W/16)\n",
    "        dec4 = self.decoder4(torch.cat([F.interpolate(bridge, size=enc4.shape[2:], mode=\"bilinear\", align_corners=True), enc4], dim=1))\n",
    "        dec3 = self.decoder3(torch.cat([F.interpolate(dec4, size=enc3.shape[2:], mode=\"bilinear\", align_corners=True), enc3], dim=1))\n",
    "        dec2 = self.decoder2(torch.cat([F.interpolate(dec3, size=enc2.shape[2:], mode=\"bilinear\", align_corners=True), enc2], dim=1))\n",
    "        dec1 = self.decoder1(torch.cat([F.interpolate(dec2, size=enc1.shape[2:], mode=\"bilinear\", align_corners=True), enc1], dim=1))\n",
    "        output = self.final_conv(dec1)\n",
    "        output = F.interpolate(output, size=(480, 480), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-06T09:48:33.158151Z",
     "iopub.status.busy": "2025-03-06T09:48:33.157815Z",
     "iopub.status.idle": "2025-03-06T09:48:44.542910Z",
     "shell.execute_reply": "2025-03-06T09:48:44.541514Z",
     "shell.execute_reply.started": "2025-03-06T09:48:33.158100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        preds = F.softmax(preds, dim=1) \n",
    "        targets_one_hot = F.one_hot(targets, num_classes=preds.shape[1]).permute(0, 3, 1, 2)\n",
    "\n",
    "        intersection = (preds * targets_one_hot).sum(dim=(2, 3))\n",
    "        union = (preds + targets_one_hot).sum(dim=(2, 3)) - intersection\n",
    "        iou = (intersection + self.eps) / (union + self.eps)\n",
    "        return 1 - iou.mean() \n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.iou_loss = IoULoss()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        ce = self.ce_loss(preds, targets)\n",
    "        iou = self.iou_loss(preds, targets)\n",
    "        return self.alpha * ce + (1 - self.alpha) * iou\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.4),\n",
    "    A.VerticalFlip(p=0.4),\n",
    "    A.RandomGamma(gamma_limit=(70, 130), p=0.2),\n",
    "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "lr = 0.005\n",
    "batch_size = 8\n",
    "epochs = 150\n",
    "in_channels = 3\n",
    "out_channels = 3  \n",
    "H, W = 480, 480 \n",
    "\n",
    "model = UNet(num_classes=out_channels)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = CombinedLoss(alpha=0.5) \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160, 100, 20])\n",
    "        upper_red2 = np.array([179, 255, 255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1  # Class 1\n",
    "        \n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2  # Class 2\n",
    "        \n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.read_mask(label_path)\n",
    "        image = cv2.resize(image, self.resize)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed['image'].float()\n",
    "            label = transformed['mask'].long() \n",
    "        \n",
    "        return image, label\n",
    "train_dataset = CustomDataset(\n",
    "    img_dir='/kaggle/input/bkai-igh-neopolyp/train/train',\n",
    "    label_dir='/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt',\n",
    "    resize=(H, W),\n",
    "    transform=train_transform,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "\n",
    "    for images, masks in progress_bar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks) \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    scheduler.step(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-06T09:48:44.543439Z",
     "iopub.status.idle": "2025-03-06T09:48:44.543679Z",
     "shell.execute_reply": "2025-03-06T09:48:44.543582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/Unet_segmentation.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-06T09:48:44.544252Z",
     "iopub.status.idle": "2025-03-06T09:48:44.544580Z",
     "shell.execute_reply": "2025-03-06T09:48:44.544457Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def infer(model, image_path, device):\n",
    "    model.eval()\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    transformed = A.Compose([\n",
    "        A.Resize(480, 640),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])(image=image)\n",
    "    \n",
    "    input_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        prediction = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(prediction, cmap=\"jet\")\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "model = UNet(num_classes=3).to(device)\n",
    "model.load_state_dict(torch.load(\"/kaggle/working/Unet_polyp_segmentation.pth\", map_location=device))\n",
    "model.eval()\n",
    "device=\"cuda\"\n",
    "infer(model, \"/kaggle/input/bkai-igh-neopolyp/train/train/0081835cf877e004e8bfb905b78a9139.jpeg\", device)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
